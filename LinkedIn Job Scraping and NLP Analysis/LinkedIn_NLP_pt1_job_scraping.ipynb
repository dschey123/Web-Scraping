{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43faca-cc69-4820-9a19-be53d06f8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ssl\n",
    "import requests\n",
    "from urllib.request import Request,urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969b748-a981-4d3c-a620-d684d7b46f19",
   "metadata": {},
   "source": [
    "# Scraping for Data Scientist Jobs in New York City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa67cd-d24d-4ff3-b6db-fe307ad43936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2c4e4-74c9-4907-87a6-1ab17e8ef6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL template with a placeholder for the start parameter\n",
    "base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Data%2BScientist&location=New%2BYork%2C%2BNew%2BYork%2C%2BUnited%2BStates&start={}\"\n",
    "\n",
    "# Initialize an empty list to store job IDs\n",
    "id_list = []\n",
    "\n",
    "# Define the number of pages you want to scrape\n",
    "num_pages = 250  # Example: Scrape 20 pages\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(num_pages):\n",
    "    start = page * 25  # Calculate the start parameter for each page\n",
    "    url = base_url.format(start)  # Construct the URL\n",
    "    response = requests.get(url)  # Fetch the data\n",
    "    list_data = response.text\n",
    "    list_soup = BeautifulSoup(list_data, \"html.parser\")\n",
    "    page_jobs = list_soup.find_all(\"li\")\n",
    "\n",
    "    for job in page_jobs:\n",
    "        base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "        if base_card_div:  # Check if the div exists to avoid errors\n",
    "            job_id = base_card_div.get('data-entity-urn').split(':')[3]\n",
    "            id_list.append(job_id)\n",
    "\n",
    "# Print the collected job IDs\n",
    "#print(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab134a7-ae3d-4f0f-8489-77fe988f6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = []\n",
    "\n",
    "# Function to fetch job details for a given job ID - iterate through list of IDs scraped\n",
    "def fetch_job_details(job_id):\n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            job_response = requests.get(job_url, timeout=10)\n",
    "            if job_response.status_code == 200:\n",
    "                job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
    "                job_post = {}\n",
    "\n",
    "                # Job Title\n",
    "                try:\n",
    "                    job_post['job_title'] = job_soup.find('h2', {'class': \"top-card-layout__title\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['job_title'] = None\n",
    "                    \n",
    "                # Job Link\n",
    "                try:\n",
    "                    job_post['job_link'] = f'https://www.linkedin.com/jobs/collections/recommended/?currentJobId={job_id}'\n",
    "                except AttributeError:\n",
    "                    job_post['job_link'] = None\n",
    "\n",
    "                # Company Name\n",
    "                try:\n",
    "                    job_post['company_name'] = job_soup.find('a', {'class': \"topcard__org-name-link\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['company_name'] = None\n",
    "\n",
    "                # Time Posted\n",
    "                try:\n",
    "                    job_post['time_posted'] = job_soup.find('span', {'class': \"posted-time-ago__text\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['time_posted'] = None\n",
    "\n",
    "                # Number of Applicants\n",
    "                try:\n",
    "                    job_post['number_applicants'] = job_soup.find('span', {'class': \"num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet\"}).text.strip()\n",
    "                    if not job_post['number_applicants']:\n",
    "                        raise AttributeError\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        job_post['number_applicants'] = job_soup.find('figcaption', {'class': \"num-applicants__caption\"}).text.strip()\n",
    "                    except AttributeError:\n",
    "                        job_post['number_applicants'] = None\n",
    "\n",
    "                # Salary\n",
    "                try:\n",
    "                    job_post['salary'] = job_soup.find('div', {'class': \"salary\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['salary'] = None\n",
    "                #Job Description\n",
    "                try:\n",
    "                    job_post['company_description'] = job_soup.find('div', {'class': \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['company_description'] = None\n",
    "    \n",
    "                return job_post\n",
    "            \n",
    "            else:\n",
    "                logging.warning(f\"Failed to fetch job ID {job_id}: HTTP {job_response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error fetching job ID {job_id}: {e}\")\n",
    "        time.sleep(5)  # Delay before retrying\n",
    "    return None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbfc7c63-7034-4cb7-bc66-06a913128a2c",
   "metadata": {},
   "source": [
    "# Additional Company Description if needed - separates by HTML tags\n",
    "                try:\n",
    "                    main_div = job_soup.find('div', {'class': \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\"})\n",
    "                    if main_div:\n",
    "                        # Extract all <strong> elements and their subsequent <ul> sections\n",
    "                        strong_elements = main_div.find_all('strong')\n",
    "                        strong_texts = []\n",
    "                        for strong in strong_elements:\n",
    "                            text = strong.text.strip()\n",
    "                            ul = strong.find_next('ul')\n",
    "                            ul_items = [li.text.strip() for li in ul.find_all('li')] if ul else []\n",
    "                            strong_texts.append({'text': text, 'ul_items': ul_items})\n",
    "                        \n",
    "                        # Extract all <br> elements and get the text before each <br>\n",
    "                        br_elements = main_div.find_all('br')\n",
    "                        br_texts = []\n",
    "                        for br in br_elements:\n",
    "                            text_before_br = br.previous_sibling\n",
    "                            if text_before_br and isinstance(text_before_br, str):\n",
    "                                br_texts.append(text_before_br.strip())\n",
    "                        \n",
    "                        job_post['company_description'] = {\n",
    "                            'strong_sections': strong_texts,\n",
    "                            'br_sections': br_texts\n",
    "                        }\n",
    "                    else:\n",
    "                        job_post['company_description'] = None\n",
    "                except AttributeError:\n",
    "                    job_post['company_description'] = None\n",
    "                    \n",
    "                try:\n",
    "                    job_post['additional'] = job_soup.find('div', {'class': \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_post['company_description1'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d01337-fd33-440c-98ca-6fa7e9da86a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each job ID\n",
    "for job_id in id_list:\n",
    "    logging.info(f\"Fetching details for job ID {job_id}\")\n",
    "    job_details = fetch_job_details(job_id)\n",
    "    if job_details:\n",
    "        job_list.append(job_details)\n",
    "    time.sleep(2)  # Delay between requests to avoid rate limiting\n",
    "\n",
    "# Print the collected job details\n",
    "#for job in job_list:\n",
    " #   print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506400a-e47e-47a1-9f00-1b81ffd52ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4ac13-595a-44d8-8be7-ba31ac14cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edc2ce-0c5a-4c83-869b-654bac90b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('linkedin_job_scrape_only_data_scientist_roles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b752286-0635-45c7-900f-d20cfb30434c",
   "metadata": {},
   "source": [
    "# Scraping for Additional Similar Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435509b-298c-4abd-8ff1-fe3c8c65c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store new job IDs\n",
    "id_list_2 = []\n",
    "job_list_2 = []\n",
    "\n",
    "# List of job titles similar to Data Scientists\n",
    "job_titles = [\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"Data Analyst\",\n",
    "    \"Software Engineer\",\n",
    "    \"Data Engineer\",\n",
    "    \"Business Intelligence Analyst\",\n",
    "    \"AI Specialist\",\n",
    "    \"Big Data Engineer\",\n",
    "    \"Quantitative Analyst\",\n",
    "    \"Research Scientist\",\n",
    "    \"Data Architect\",\n",
    "    \"Statistician\",\n",
    "    \"Predictive Modeler\",\n",
    "    \"Operations Research Analyst\",\n",
    "    \"Decision Scientist\",\n",
    "    \"Data Visualization Specialist\",\n",
    "    \"Analytics Consultant\"\n",
    "]\n",
    "\n",
    "# Function to format job titles for URL\n",
    "def format_job_title(title):\n",
    "    return title.replace(\" \", \"%2B\")\n",
    "\n",
    "# Define the base URL with placeholders for the job title and the start parameter\n",
    "base_url_template = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={}&location=New%2BYork%2C%2BNew%2BYork%2C%2BUnited%2BStates&start={}\"\n",
    "\n",
    "# Define the number of pages you want to scrape\n",
    "num_pages_2 = 250  # Example: Scrape 50 pages\n",
    "\n",
    "# Loop through each job title\n",
    "for job_title in job_titles:\n",
    "    formatted_title = format_job_title(job_title)\n",
    "    for page in range(num_pages_2):\n",
    "        start_2 = page * 25  # Calculate the start parameter for each page\n",
    "        url_2 = base_url_template.format(formatted_title, start_2)  # Construct the URL\n",
    "        response_2 = requests.get(url_2)  # Fetch the data\n",
    "        list_data_2 = response_2.text\n",
    "        list_soup_2 = BeautifulSoup(list_data_2, \"html.parser\")\n",
    "        page_jobs_2 = list_soup_2.find_all(\"li\")\n",
    "\n",
    "        for job in page_jobs_2:\n",
    "            base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "            if base_card_div:  # Check if the div exists to avoid errors\n",
    "                job_id_2 = base_card_div.get('data-entity-urn').split(':')[3]\n",
    "                id_list_2.append(job_id_2)\n",
    "\n",
    "# Output the collected job IDs\n",
    "#print(id_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54d59c-2fea-448d-9b27-8fc72ff6672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each job ID\n",
    "for job_id in id_list_2:\n",
    "    logging.info(f\"Fetching details for job ID {job_id}\")\n",
    "    job_details_2 = fetch_job_details(job_id)\n",
    "    if job_details_2:\n",
    "        job_list_2.append(job_details_2)\n",
    "    time.sleep(2)  # Delay between requests to avoid rate limiting\n",
    "\n",
    "# Print the collected job details\n",
    "#for job in job_list:\n",
    " #   print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50b1d0-61f8-4077-91ef-e8dc41ef1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output DataFrame for additional IDs/roles scraped\n",
    "df1=pd.DataFrame(job_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56bf71-9f00-430e-af70-6d556de90d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86ca8b-f389-49fe-ac0e-9e501366a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat the two DataFrames into one to analyze\n",
    "df_final=pd.concat([df,df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04f5a0-55a3-43df-8d2e-ed126943167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc03f4-1b87-4bc5-abb9-fdee8ef88ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('linkedin_job_scrape_all_roles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
