{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fb8e2-cf93-40cb-a887-9132cf2f5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "#import torch\n",
    "#from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim import corpora, models\n",
    "from gensim.models import ldamodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bf4d1-6d21-473a-ac21-6b6f555dcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of words in each predefined cateogry (combine w sentiment analysis -/+)\n",
    "#categorize review based on count/sentiment analysis\n",
    "#then do topic modeling,popular keyword count,named entity recognition on each of the categorized review categories \n",
    "#retrain and add the popular words back into predefined list\n",
    "#categorize reviews based on keyword prevalence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d891bbd-9e83-4f67-b520-086677736616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f67dc-8ae7-41b4-bd52-c13e127d679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fritto.csv',index_col=0)\n",
    "df=df.drop_duplicates(['Name','Date Posted'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d7de7-8c33-4e97-bee0-17eff17b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_reviews=df[df['Ratings']<4]\n",
    "#for i in df_bad_reviews['Reviews']:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea677c16-f6d9-4186-ac71-d7df8ad1f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(review):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(review)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and len(word) >= 3]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['processed_reviews'] = df['Reviews'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce336282-38d3-4569-bd0d-aa07146f98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be93f82-d5dc-451f-b319-2ffdf7efe39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define categories\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample restaurant reviews\n",
    "reviews=df['processed_reviews'].to_list()\n",
    "\n",
    "# Preprocessing steps\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    tokens = word_tokenize(review.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]  # Lemmatization and removing stopwords\n",
    "    preprocessed_reviews.append(\" \".join(filtered_tokens))\n",
    "\n",
    "# Count Vectorization\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Limit the number of features for demonstration\n",
    "X = vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n",
    "# LDA Modeling\n",
    "num_topics = 15  # Number of topics/themes\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display topics and their top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-5 - 1:-1]]  # Display top 5 words per topic\n",
    "    print(f\"Topic {topic_idx + 1}: {' | '.join(top_words)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44716fcb-ad17-4d7f-a6b1-08e7c21998df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Assigned Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b14fe5-f8d0-4bee-ace0-a83699d6a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count x sentiment x similarity - no negative keyword emphasis - 1 (apply to negative keyword emphasis)\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from textblob import TextBlob  # For sentiment analysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove non-alphanumeric characters\n",
    "    return tokens\n",
    "\n",
    "reviews=df['processed_reviews'].to_list()\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]\n",
    "\n",
    "# Train Word2Vec model on the preprocessed reviews\n",
    "word2vec_model = Word2Vec(processed_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define the main words for each predefined topic along with additional related words (in lowercase)\n",
    "predefined_topics = {\n",
    "    \"service\": ['host','listen','listening','delivery','call','wait','right away','service','slow','time','menu','hostess','table','seat','host','waiter','table','receptionist','quick','responsive','accommodating','waitress','seated','rude','took','order','ordering',\"service\",'min','minutes','server', \"excellent\",'check', \"staff\", \"attentive\",\"friendly\", \"helpful\",'courteous', \"prompt\",'mean','answer', \"efficient\", \"knowledgeable\",'phone','polite'],\n",
    "    \"ambiance\": [\"ambiance\",'clean','dirty','bathrooms','decor','atmosphere','temperature','AC','tables','toilet','kitchen','party','place','family','casual','formal','romantic','spot','festive','lively','intimate','loud','noisy','bar','seats','chairs','location', \"atmosphere\", \"decor\", \"cozy\", \"inviting\", \"warm\", \"stylish\", \"elegant\", \"relaxed\",'parking','building','seating','park','lights'],\n",
    "    \"prices\": [\"prices\",'price','total','bill was','check was','worth','value','money','pricey','pricy','portions','portion size','quantity', \"reasonable\",'over-priced','over','priced','loaded', \"affordable\",'inexpensive','wallet', \"cost\", \"expensive\", \"cheap\", \"wallet-friendly\",\"large portions\",\"large plate\"],\n",
    "    \"quality\": ['quality','lemon','runny','lime','takeout','vegan','gnocchi','vegetarian','fresh','well-prepared','stale','well','prepared','authentic','atomic','calamari','pillows','puffs','cheese','ricotta','dish','dessert','inedible','flourless','tiramisu','satisfying','caprese','turkey','steak','chicken','sausage','lasagna','jumbo','like','pasta','salt','bread','shrimp','chocolate','cake','cream','drinks','wine', \"delicious\",'taste', \"tasty\", \"flavorful\",'runny','flavorless','flavor',\"fresh\",'disgusting','penne','fettuccine','alfredo', \"high-quality\",'prepared', \"well-prepared\",'garlic','bland','unflavorful','oily','oil','greasy','cream','heavy','sauce','fried','filling','sauces','ravioli'],\n",
    "    \"overall\": ['overall','fritto misto','santa monica','la','los angeles','card','somewhere different','somewhere else','course','refund','reviews','review',\"best spot\",'food','come again','avoid','meal','menu','back','return','choice','elsewhere', \"experience\", \"visit\",'would not return', \"would return\",'come back','place','italian','restaurant','website','app', \"would recommend to a friend\", \"would return\", \"would not return\",'people','crowd','italian','authentic','waste'],\n",
    "}\n",
    "\n",
    "# Negative sentiment words\n",
    "negative_words = [\n",
    "    \"bad\", \"horrible\", \"terrible\", \"poor\", \"disgusting\",\n",
    "    \"awful\", \"unpleasant\", \"dreadful\", \"unappetizing\",\n",
    "    \"disappointing\", \"mediocre\", \"unpalatable\", \"inferior\",\n",
    "    \"overpriced\", \"unsatisfactory\", \"disastrous\", \"unimpressive\",\n",
    "    \"bland\", \"gross\", \"repulsive\", \"underwhelming\",\n",
    "    \"inedible\", \"unflavorful\", \"miserable\", \"regrettable\",\n",
    "    \"subpar\", \"lousy\", \"offensive\", \"unenjoyable\",\n",
    "    \"stale\", \"inedible\", \"undercooked\", \"soggy\",\"horrendous\"]\n",
    "\n",
    "negative_word_weights = {\n",
    "    \"bad\": 1.0,\n",
    "    \"horrible\": 2.0,\n",
    "    \"Horrendous\":2.0,\n",
    "    \"terrible\": 2.0,\n",
    "    \"poor\": 1.0,\n",
    "    \"disgusting\": 2.0,\n",
    "    \"awful\": 1.2,\n",
    "    \"unpleasant\": 1.3,\n",
    "    \"dreadful\": 1.5,\n",
    "    \"unappetizing\": 1.2,\n",
    "    \"disappointing\": 1.2,\n",
    "    \"mediocre\": 1.1,\n",
    "    \"unpalatable\": 1.2,\n",
    "    \"inferior\": 1.0,\n",
    "    \"overpriced\": 1.2,\n",
    "    \"unsatisfactory\": 1.1,\n",
    "    \"disastrous\": 1.5,\n",
    "    \"unimpressive\": 1.1,\n",
    "    \"bland\": 1.0,\n",
    "    \"gross\": 1.2,\n",
    "    \"repulsive\": 1.4,\n",
    "    \"underwhelming\": 1.1,\n",
    "    \"inedible\": 1.3,\n",
    "    \"unflavorful\": 1.2,\n",
    "    \"miserable\": 1.3,\n",
    "    \"regrettable\": 1.2,\n",
    "    \"subpar\": 1.1,\n",
    "    \"lousy\": 1.2,\n",
    "    \"offensive\": 1.3,\n",
    "    \"unenjoyable\": 1.2,\n",
    "    \"stale\": 1.1,\n",
    "    \"undercooked\": 1.2,\n",
    "    \"soggy\": 1.2,\n",
    "}\n",
    "\n",
    "# Transition words for sentence transitions\n",
    "sentence_transitions = ['but', 'and', 'however', 'although', 'yet','so', 'nevertheless', 'while', 'though', 'or']\n",
    "\n",
    "\n",
    "def calculate_sentiment(review):\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review]\n",
    "    sentiment_distribution = [score - 1 for score in sentiment_scores]  # Shift to negative range for negative sentiment\n",
    "    sentiment_distribution /= np.sum(np.abs(sentiment_distribution))  # Normalize\n",
    "    sentiment = np.argmax(sentiment_distribution)\n",
    "\n",
    "    # Adjust the sentiment score based on the order of the words and the context\n",
    "    for i in range(len(sentiment_scores)):\n",
    "        if sentiment_scores[i] > 0:\n",
    "            sentiment_distribution[i] *= (-1) ** (i + 1)\n",
    "\n",
    "    sentiment = np.argmax(sentiment_distribution)\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "def calculate_sentiment_entropy(review):\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review]\n",
    "    entropy = -sum(score * np.log2(score) for score in sentiment_scores if score != 0)\n",
    "    return entropy\n",
    "\n",
    "def get_sentiment(word):\n",
    "    return TextBlob(word).sentiment.polarity\n",
    "\n",
    "def get_main_topic(review):\n",
    "    review_text = ' '.join(review)  # Convert preprocessed words back into a string\n",
    "    sentiment_scores = [get_sentiment(word) for word in review]  # Define sentiment_scores here\n",
    "    sentiment = calculate_sentiment(review_text)\n",
    "    \n",
    "    negative_sentiment_words = [word for word in review if word in negative_words]\n",
    "    negative_sentiment_scores = [get_sentiment(word) for word in negative_sentiment_words]\n",
    "    \n",
    "    review_embedding = np.nanmean(\n",
    "        [word2vec_model.wv[word] for word in review if word in word2vec_model.wv.key_to_index], axis=0\n",
    "    )\n",
    "    \n",
    "    if np.any(np.isnan(review_embedding)):\n",
    "        return None\n",
    "    \n",
    "    max_score = float('-inf')\n",
    "    main_topic = None\n",
    "    second_topic = None\n",
    "    highest_negative_score = 0\n",
    "    prioritized_categories = []\n",
    "    \n",
    "    for topic, words in predefined_topics.items():\n",
    "        count = sum(1 for word in words if word in review)\n",
    "\n",
    "        topic_embedding = np.nanmean(\n",
    "            [word2vec_model.wv[word] for word in words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "        )\n",
    "\n",
    "        if np.any(np.isnan(topic_embedding)):\n",
    "            continue\n",
    "\n",
    "        similarity = cosine_similarity([review_embedding], [topic_embedding])[0][0]\n",
    "\n",
    "        # Prioritize topics containing negative sentiment keywords\n",
    "        negative_sentiment_score = sum(1 for word in review if word in negative_words)\n",
    "        if negative_sentiment_score > 0 and any(word in words for word in negative_words):\n",
    "            combined_score = similarity + (0.5 * negative_sentiment_score) + (0.5 * count)\n",
    "        else:\n",
    "            combined_score = similarity + (0.5 * sentiment) + (0.5 * count)\n",
    "\n",
    "        if combined_score > max_score:\n",
    "            second_topic = main_topic\n",
    "            max_score = combined_score\n",
    "            main_topic = topic\n",
    "            prioritized_categories = [topic]\n",
    "        elif combined_score == max_score:\n",
    "            second_topic = topic\n",
    "    sentiment_entropy = calculate_sentiment_entropy(review)\n",
    "    if sentiment_entropy > 1.0:\n",
    "        return main_topic\n",
    "    #f\"{main_topic}, {second_topic}\"\n",
    "    else:\n",
    "        return main_topic\n",
    "\n",
    "\n",
    "# Assign each review to one of the predefined topics\n",
    "review_topics = [get_main_topic(review) for review in processed_reviews]\n",
    "\n",
    "# Print the assigned topics for each review along with scores and negative word information\n",
    "for i, review in enumerate(reviews):\n",
    "    main_topic = review_topics[i]\n",
    "    count_negative_words = sum(1 for word in preprocess_text(review) if word in negative_words)\n",
    "    negative_word_score = sum(negative_word_weights.get(word, 1.0) for word in preprocess_text(review) if word in negative_words)\n",
    "    \n",
    "    review_words = preprocess_text(review)\n",
    "    review_embedding = np.nanmean(\n",
    "        [word2vec_model.wv[word] for word in review_words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "    )\n",
    "    \n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review_words]\n",
    "    sentiment = np.argmax([score + 1 for score in sentiment_scores])  # Shift to positive range\n",
    "    review_data = {\n",
    "        'Assigned Topic': main_topic,\n",
    "        'Count of Negative Keywords': count_negative_words,\n",
    "        'Aggregate Negative Score': negative_word_score\n",
    "    }\n",
    "    for category, words in predefined_topics.items():\n",
    "        topic_embedding = np.nanmean(\n",
    "            [word2vec_model.wv[word] for word in words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "        )\n",
    "        \n",
    "        if np.any(np.isnan(topic_embedding)):\n",
    "            category_score = \"N/A\"\n",
    "        else:\n",
    "            similarity = cosine_similarity([review_embedding], [topic_embedding])[0][0]\n",
    "            sentiment_score = (0.5 * sentiment) if any(word in words for word in review_words) else 0\n",
    "            category_score = similarity + sentiment_score\n",
    "            if category == 'service':\n",
    "                review_data['Service Score'] = category_score\n",
    "            elif category == 'ambiance':\n",
    "                review_data['Ambiance Score'] = category_score\n",
    "            elif category == 'prices':\n",
    "                review_data['Prices Score'] = category_score\n",
    "            elif category == 'quality':\n",
    "                review_data['Quality Score'] = category_score\n",
    "            elif category == 'overall':\n",
    "                review_data['Overall Score'] = category_score\n",
    "    #review_data['Overall Score'] = max(\n",
    "        review_data.get('Overall Score', float('-inf')),\n",
    "        review_data.get('Service Score', float('-inf')),\n",
    "        review_data.get('Ambiance Score', float('-inf')),\n",
    "        review_data.get('Prices Score', float('-inf')),\n",
    "        review_data.get('Quality Score', float('-inf'))\n",
    "        #)\n",
    "    df.loc[df.index[i], review_data.keys()] = list(review_data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7c4de-0fb7-442c-aced-7b5caf46781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177c19e-bc1a-4a35-9745-cbefe545c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ab816-7867-4720-9396-9f92773ee6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226745a4-58e6-4609-ae69-60e0bd3a045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Assigned Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d965795-3774-46fb-955d-753282674a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Assigned Topic': ['quality', 'prices', 'overall', 'ambiance', 'service', 'quality']}\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize an empty dictionary to store the DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\"quality\", \"prices\", \"overall\", \"ambiance\", \"service\"]\n",
    "\n",
    "# Create a DataFrame for each keyword\n",
    "for keyword in keywords:\n",
    "    # Use the keyword to filter the original DataFrame\n",
    "    keyword_df = df[df['Assigned Topic'].str.lower() == keyword.lower()]\n",
    "    \n",
    "    # Store the filtered DataFrame in the dictionary\n",
    "    dataframes[f'df_{keyword}'] = keyword_df\n",
    "\n",
    "# Now, dataframes is a dictionary where each key is the name of the DataFrame (e.g., 'df_quality')\n",
    "# and the corresponding value is the filtered DataFrame.\n",
    "# You can access the DataFrames like this:\n",
    "df_quality = dataframes['df_quality']\n",
    "df_prices = dataframes['df_prices']\n",
    "df_overall = dataframes['df_overall']\n",
    "df_ambiance = dataframes['df_ambiance']\n",
    "df_service = dataframes['df_service']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a107ec-6edd-4e01-8367-2ef492b86551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1063a55-7a0b-4425-8ccf-b5835a181862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af90208-0eae-4c42-b420-edc510e9acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to associate DataFrame names with actual DataFrames\n",
    "dataframe_dict = {\n",
    "    'df_quality': df_quality,\n",
    "    'df_prices': df_prices,\n",
    "    'df_overall': df_overall,\n",
    "    'df_ambiance': df_ambiance,\n",
    "    'df_service': df_service\n",
    "}\n",
    "\n",
    "# Create a function to preprocess and analyze the reviews\n",
    "def identify_keywords(review_text):\n",
    "    words = word_tokenize(review_text.lower())\n",
    "    words = [word for word in words if word is not None and word.isalpha() and word not in stopwords.words('english') and word not in [\"get\", \"back\", \"really\"]]\n",
    "    word_count = Counter(words)\n",
    "    return word_count\n",
    "\n",
    "# Function to extract nouns based on part-of-speech tagging\n",
    "def extract_nouns(word_list):\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    nouns = [word for word, tag in tagged_words if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return nouns\n",
    "\n",
    "# Create variables to store the most common nouns for each DataFrame\n",
    "most_common_nouns_df_quality = []\n",
    "most_common_nouns_df_prices = []\n",
    "most_common_nouns_df_overall = []\n",
    "most_common_nouns_df_ambiance = []\n",
    "most_common_nouns_df_service = []\n",
    "\n",
    "# Iterate through each DataFrame in the dictionary\n",
    "for df_name, df in dataframe_dict.items():\n",
    "    keywords = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        review_text = row['Reviews']\n",
    "        word_count = identify_keywords(review_text)\n",
    "\n",
    "        # Append the words to the list of keywords\n",
    "        keywords.extend(word_count.keys())\n",
    "\n",
    "    # Extract nouns based on part-of-speech tagging and remove common stopwords\n",
    "    keywords = extract_nouns(keywords)\n",
    "\n",
    "    # Count the frequency of each noun across the DataFrame\n",
    "    keyword_counts = Counter(keywords)\n",
    "\n",
    "    # Store the top 10 nouns in the corresponding variable\n",
    "    if df_name == 'df_quality':\n",
    "        most_common_nouns_df_quality = [word for word, count in keyword_counts.most_common(50)]\n",
    "    elif df_name == 'df_prices':\n",
    "        most_common_nouns_df_prices = [word for word, count in keyword_counts.most_common(50)]\n",
    "    elif df_name == 'df_overall':\n",
    "        most_common_nouns_df_overall = [word for word, count in keyword_counts.most_common(50)]\n",
    "    elif df_name == 'df_ambiance':\n",
    "        most_common_nouns_df_ambiance = [word for word, count in keyword_counts.most_common(50)]\n",
    "    elif df_name == 'df_service':\n",
    "        most_common_nouns_df_service = [word for word, count in keyword_counts.most_common(50)]\n",
    "\n",
    "# Now, you can access the lists as variables, e.g., most_common_nouns_df_quality, most_common_nouns_df_prices, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f956c53-3376-4f3d-9001-ccb4e53ad2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists = [most_common_nouns_df_quality, most_common_nouns_df_prices, most_common_nouns_df_overall, most_common_nouns_df_ambiance, most_common_nouns_df_service]\n",
    "# Create a set to store common words\n",
    "common_words = set()\n",
    "\n",
    "# Iterate through all pairs of lists to find common words\n",
    "for i in range(len(all_lists)):\n",
    "    for j in range(i + 1, len(all_lists)):\n",
    "        common_words |= set(all_lists[i]) & set(all_lists[j])\n",
    "\n",
    "# Remove common words from each list\n",
    "for i in range(len(all_lists)):\n",
    "    all_lists[i] = [word for word in all_lists[i] if word not in common_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e58f4-fbec-451c-8354-42ec8f0fea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_nouns_df_quality=all_lists[0]\n",
    "most_common_nouns_df_prices=all_lists[1]\n",
    "most_common_nouns_df_overall=all_lists[2]\n",
    "most_common_nouns_df_ambiance=all_lists[3]\n",
    "most_common_nouns_df_service=all_lists[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc4a75-6fce-475c-852b-23e543e992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_values_to_remove = ['lot', 'bit','home','street']\n",
    "most_common_nouns_df_quality = [x for x in most_common_nouns_df_quality if x not in qual_values_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c04c9c-7ec4-48f3-816e-219f3d92db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_values_to_remove = ['house', 'nothing','kind','beach','thing','disappoint']\n",
    "most_common_nouns_df_overall = [x for x in most_common_nouns_df_overall if x not in overall_values_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715c741-46ea-43cd-9de9-153548624e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_values_to_remove = ['meals', 'carbonara','carbo','lasagna','ones','minestrone','cajun','things','daughter','hunger','one']\n",
    "most_common_nouns_df_ambiance = [x for x in most_common_nouns_df_ambiance if x not in amb_values_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb7645-6275-4316-bc48-dcdafaa52783",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_nouns_df_service\n",
    "serv_values_to_remove = ['part', 'way','weekend','try','someone','lunch']\n",
    "most_common_nouns_df_service = [x for x in most_common_nouns_df_service if x not in serv_values_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732277-43ef-48d6-b44c-bef9ecef0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append back to predefined topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea2b4b-dd85-4897-b5f5-d24acb9de77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "predefined_topics = predefined_topics.copy()  # Copy the predefined_topics\n",
    "\n",
    "# Iterate through your lists and append words to the corresponding predefined_topics keys\n",
    "for keyword, word_list in [(\"service\", most_common_nouns_df_service), (\"quality\", most_common_nouns_df_quality), (\"ambiance\", most_common_nouns_df_ambiance), (\"prices\", most_common_nouns_df_prices)]:\n",
    "    for word in word_list:\n",
    "        if word not in predefined_topics[keyword]:\n",
    "            predefined_topics[keyword].append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a3fed-1500-4943-8413-312bba2f5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerun the model\n",
    "#word count x sentiment x similarity - no negative keyword emphasis - 1 (apply to negative keyword emphasis)\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from textblob import TextBlob  # For sentiment analysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove non-alphanumeric characters\n",
    "    return tokens\n",
    "\n",
    "reviews=df_copy['processed_reviews'].to_list()\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]\n",
    "\n",
    "# Train Word2Vec model on the preprocessed reviews\n",
    "word2vec_model = Word2Vec(processed_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Negative sentiment words\n",
    "negative_words = [\n",
    "    \"bad\", \"horrible\", \"terrible\", \"poor\", \"disgusting\",\n",
    "    \"awful\", \"unpleasant\", \"dreadful\", \"unappetizing\",\n",
    "    \"disappointing\", \"mediocre\", \"unpalatable\", \"inferior\",\n",
    "    \"overpriced\", \"unsatisfactory\", \"disastrous\", \"unimpressive\",\n",
    "    \"bland\", \"gross\", \"repulsive\", \"underwhelming\",\n",
    "    \"inedible\", \"unflavorful\", \"miserable\", \"regrettable\",\n",
    "    \"subpar\", \"lousy\", \"offensive\", \"unenjoyable\",\n",
    "    \"stale\", \"inedible\", \"undercooked\", \"soggy\",\"horrendous\"]\n",
    "\n",
    "negative_word_weights = {\n",
    "    \"bad\": 1.0,\n",
    "    \"horrible\": 2.0,\n",
    "    \"Horrendous\":2.0,\n",
    "    \"terrible\": 2.0,\n",
    "    \"poor\": 1.0,\n",
    "    \"disgusting\": 2.0,\n",
    "    \"awful\": 1.2,\n",
    "    \"unpleasant\": 1.3,\n",
    "    \"dreadful\": 1.5,\n",
    "    \"unappetizing\": 1.2,\n",
    "    \"disappointing\": 1.2,\n",
    "    \"mediocre\": 1.1,\n",
    "    \"unpalatable\": 1.2,\n",
    "    \"inferior\": 1.0,\n",
    "    \"overpriced\": 1.2,\n",
    "    \"unsatisfactory\": 1.1,\n",
    "    \"disastrous\": 1.5,\n",
    "    \"unimpressive\": 1.1,\n",
    "    \"bland\": 1.0,\n",
    "    \"gross\": 1.2,\n",
    "    \"repulsive\": 1.4,\n",
    "    \"underwhelming\": 1.1,\n",
    "    \"inedible\": 1.3,\n",
    "    \"unflavorful\": 1.2,\n",
    "    \"miserable\": 1.3,\n",
    "    \"regrettable\": 1.2,\n",
    "    \"subpar\": 1.1,\n",
    "    \"lousy\": 1.2,\n",
    "    \"offensive\": 1.3,\n",
    "    \"unenjoyable\": 1.2,\n",
    "    \"stale\": 1.1,\n",
    "    \"undercooked\": 1.2,\n",
    "    \"soggy\": 1.2,\n",
    "}\n",
    "\n",
    "# Transition words for sentence transitions\n",
    "sentence_transitions = ['but', 'and', 'however', 'although', 'yet','so', 'nevertheless', 'while', 'though', 'or']\n",
    "\n",
    "\n",
    "def calculate_sentiment(review):\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review]\n",
    "    sentiment_distribution = [score - 1 for score in sentiment_scores]  # Shift to negative range for negative sentiment\n",
    "    sentiment_distribution /= np.sum(np.abs(sentiment_distribution))  # Normalize\n",
    "    sentiment = np.argmax(sentiment_distribution)\n",
    "\n",
    "    # Adjust the sentiment score based on the order of the words and the context\n",
    "    for i in range(len(sentiment_scores)):\n",
    "        if sentiment_scores[i] > 0:\n",
    "            sentiment_distribution[i] *= (-1) ** (i + 1)\n",
    "\n",
    "    sentiment = np.argmax(sentiment_distribution)\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "def calculate_sentiment_entropy(review):\n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review]\n",
    "    entropy = -sum(score * np.log2(score) for score in sentiment_scores if score != 0)\n",
    "    return entropy\n",
    "\n",
    "def get_sentiment(word):\n",
    "    return TextBlob(word).sentiment.polarity\n",
    "\n",
    "def get_main_topic(review):\n",
    "    review_text = ' '.join(review)  # Convert preprocessed words back into a string\n",
    "    sentiment_scores = [get_sentiment(word) for word in review]  # Define sentiment_scores here\n",
    "    sentiment = calculate_sentiment(review_text)\n",
    "\n",
    "    negative_sentiment_words = [word for word in review if word in negative_words]\n",
    "    negative_sentiment_scores = [get_sentiment(word) for word in negative_sentiment_words]\n",
    "\n",
    "    review_embedding = np.nanmean(\n",
    "        [word2vec_model.wv[word] for word in review if word in word2vec_model.wv.key_to_index], axis=0\n",
    "    )\n",
    "\n",
    "    if np.any(np.isnan(review_embedding)):\n",
    "        return None\n",
    "\n",
    "    max_score = float('-inf')\n",
    "    main_topic = None\n",
    "    second_max_score = float('-inf')\n",
    "    second_topic = None\n",
    "    highest_negative_score = 0\n",
    "    prioritized_categories = []\n",
    "\n",
    "    for topic, words in predefined_topics.items():\n",
    "        count = sum(1 for word in words if word in review)\n",
    "\n",
    "        topic_embedding = np.nanmean(\n",
    "            [word2vec_model.wv[word] for word in words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "        )\n",
    "\n",
    "        if np.any(np.isnan(topic_embedding)):\n",
    "            category_score = \"N/A\"\n",
    "        else:\n",
    "            similarity = cosine_similarity([review_embedding], [topic_embedding])[0][0]\n",
    "\n",
    "            # Prioritize topics containing negative sentiment keywords\n",
    "            negative_sentiment_score = sum(1 for word in review if word in negative_words)\n",
    "            if negative_sentiment_score > 0 and any(word in words for word in negative_words):\n",
    "                combined_score = similarity + (0.5 * negative_sentiment_score) + (0.5 * count)\n",
    "            else:\n",
    "                combined_score = similarity + (0.5 * sentiment) + (0.5 * count)\n",
    "\n",
    "            if combined_score > max_score:\n",
    "                # Update the second topic\n",
    "                second_max_score = max_score\n",
    "                second_topic = main_topic\n",
    "                max_score = combined_score\n",
    "                main_topic = topic\n",
    "                prioritized_categories = [topic]\n",
    "            elif combined_score == max_score:\n",
    "                # Update the second topic if the score is the same\n",
    "                second_max_score = max(second_max_score, combined_score)\n",
    "                second_topic = topic\n",
    "\n",
    "    sentiment_entropy = calculate_sentiment_entropy(review)\n",
    "    if sentiment_entropy > 1.0:\n",
    "        return f\"{main_topic}, {second_topic}\" if second_topic else main_topic\n",
    "    else:\n",
    "        return f\"{main_topic}, {second_topic}\" if second_topic else main_topic\n",
    "\n",
    "\n",
    "\n",
    "# Assign each review to one of the predefined topics\n",
    "review_topics = [get_main_topic(review) for review in processed_reviews]\n",
    "\n",
    "# Print the assigned topics for each review along with scores and negative word information\n",
    "for i, review in enumerate(reviews):\n",
    "    main_topic = review_topics[i]\n",
    "    count_negative_words = sum(1 for word in preprocess_text(review) if word in negative_words)\n",
    "    negative_word_score = sum(negative_word_weights.get(word, 1.0) for word in preprocess_text(review) if word in negative_words)\n",
    "    \n",
    "    review_words = preprocess_text(review)\n",
    "    review_embedding = np.nanmean(\n",
    "        [word2vec_model.wv[word] for word in review_words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "    )\n",
    "    \n",
    "    sentiment_scores = [TextBlob(word).sentiment.polarity for word in review_words]\n",
    "    sentiment = np.argmax([score + 1 for score in sentiment_scores])  # Shift to positive range\n",
    "    review_data = {\n",
    "        'Assigned Topic': main_topic,\n",
    "        'Count of Negative Keywords': count_negative_words,\n",
    "        'Aggregate Negative Score': negative_word_score\n",
    "    }\n",
    "    for category, words in predefined_topics.items():\n",
    "        topic_embedding = np.nanmean(\n",
    "            [word2vec_model.wv[word] for word in words if word in word2vec_model.wv.key_to_index], axis=0\n",
    "        )\n",
    "        \n",
    "        if np.any(np.isnan(topic_embedding)):\n",
    "            category_score = \"N/A\"\n",
    "        else:\n",
    "            similarity = cosine_similarity([review_embedding], [topic_embedding])[0][0]\n",
    "            sentiment_score = (0.5 * sentiment) if any(word in words for word in review_words) else 0\n",
    "            category_score = similarity + sentiment_score\n",
    "            if category == 'service':\n",
    "                review_data['Service Score'] = category_score\n",
    "            elif category == 'ambiance':\n",
    "                review_data['Ambiance Score'] = category_score\n",
    "            elif category == 'prices':\n",
    "                review_data['Prices Score'] = category_score\n",
    "            elif category == 'quality':\n",
    "                review_data['Quality Score'] = category_score\n",
    "            elif category == 'overall':\n",
    "                review_data['Overall Score'] = category_score\n",
    "    #review_data['Overall Score'] = max(\n",
    "        #review_data.get('Overall Score', float('-inf')),\n",
    "        #review_data.get('Service Score', float('-inf')),\n",
    "        #review_data.get('Ambiance Score', float('-inf')),\n",
    "        #review_data.get('Prices Score', float('-inf')),\n",
    "        #review_data.get('Quality Score', float('-inf'))\n",
    "        #)\n",
    "    df_copy.loc[df_copy.index[i], review_data.keys()] = list(review_data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229ca88-308c-4074-be02-c70fcc8d89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c184ff0-4c91-46fb-b8c2-c4762cfc1498",
   "metadata": {},
   "source": [
    "# Example Review Analysis - Jess S."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ca00a47-2d2a-458f-81f0-0e4c2c629b02",
   "metadata": {},
   "source": [
    "#1 Find a bad reivew - Make sure member lives close to LA/within CA and date posted of review was relatively recent\n",
    "#2 Find Assigned Topic for this review\n",
    "#3 Write a retargeting letter apologizing for issues in Assigned Topic - make personalized based on concerns\n",
    "#4 Add incentive based on assigned topic categories to hopefully winback customer\n",
    "#5 Review and send to customer through yelp message\n",
    "\n",
    "Subject: Our Apology and a Token of Appreciation\n",
    "\n",
    "Dear [Customer's Name],\n",
    "\n",
    "We sincerely apologize for the recent dining experience at [Restaurant Name]. Your feedback is invaluable, and we regret that we fell short in delivering our usual quality in both food and service.\n",
    "\n",
    "As a gesture of our apology, weâ€™d like to offer you a [percentage]% discount on your next visit with us within the next [timeframe, e.g., 30 days]. Please use the code [Discount Code: XXXX] when making a reservation or present this email to your server.\n",
    "\n",
    "Rest assured, we're actively addressing the issues raised to prevent recurrence. Your feedback has been shared with our teams to ensure improvements.\n",
    "\n",
    "We genuinely hope for the opportunity to welcome you back and provide the exceptional experience you deserve.\n",
    "\n",
    "Thank you for your understanding.\n",
    "\n",
    "Warm regards,\n",
    "\n",
    "[Robert Kerr]\n",
    "[Fritto Misto Owner]\n",
    "[Fritto Misto]\n",
    "[xxx-xxx-xxxx]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
