{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ssl\n",
    "import requests\n",
    "from urllib.request import Request,urlopen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "#import seaborn as sns\n",
    "import spacy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape Yelp Reviews to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yelp='https://www.yelp.com/biz/fritto-misto-santa-monica-2'\n",
    "yelp=requests.get('https://www.yelp.com/biz/fritto-misto-santa-monica-2?osq=fritto+misto')\n",
    "#print(yelp)\n",
    "html=BeautifulSoup(yelp.text,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url2 = 'https://www.yelp.com/biz/fritto-misto-santa-monica-2?osq=fritto+misto&start='\n",
    "#start = 0\n",
    "#num_pages = 10\n",
    "#end = 20* num_pages\n",
    "\n",
    "name=[]\n",
    "reviews=[]\n",
    "ratings=[]\n",
    "date=[]\n",
    "location=[]\n",
    "urllist=[]\n",
    "\n",
    "url_list=[]\n",
    "for i in tqdm(range(0,3300,10)):\n",
    "    URL = ('https://www.yelp.com/biz/fritto-misto-santa-monica-2?start='+str(i))\n",
    "    yelp=requests.get(URL)\n",
    "    html=BeautifulSoup(yelp.text,'html.parser')\n",
    "#while (start<end):\n",
    "    #url = url2+str(start)\n",
    "    #print(url)\n",
    "    #start +=10\n",
    "    #print(url)\n",
    "    rev=html.find_all('div',attrs={'class':'review__09f24__oHr9V border-color--default__09f24__NPAKY'})\n",
    "    for review in rev:\n",
    "        reviews.append(review.find('p').get_text())\n",
    "\n",
    "    rati=html.find_all('div',{\"class\":\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"})\n",
    "    for i in rati:\n",
    "        ratingshtml=i.find('span',{\"class\":\"display--inline__09f24__c6N_k border-color--default__09f24__NPAKY\"})\n",
    "        star_rating=ratingshtml.div.get('aria-label')  \n",
    "        bad_char=['star rating']\n",
    "        for rating in bad_char:\n",
    "            clean_rating=star_rating.replace(rating,\"\").strip()\n",
    "        ratings.append(clean_rating)\n",
    "        #print(ratings)\n",
    "        \n",
    "    dates=html.find_all('div',{\"class\":\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"})\n",
    "    for i in dates:\n",
    "        datehtml=i.find('span',{\"class\":\"css-chan6m\"})\n",
    "        date.append(datehtml.get_text())\n",
    "        \n",
    "    names=html.find_all('div',{\"class\":\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"})\n",
    "    for i in names:\n",
    "        namehtml=i.find('span',{\"class\":\"fs-block css-ux5mu6\"})\n",
    "        name.append(namehtml.get_text())\n",
    "    \n",
    "    loc=html.find_all('div',{\"class\":\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"})\n",
    "    for i in loc:\n",
    "        try:\n",
    "            locationhtml=i.find('span',{\"class\":\"css-qgunke\"})\n",
    "            location.append(locationhtml.get_text())\n",
    "        except AttributeError:\n",
    "            location.append(\"No Location\")\n",
    "        #print(location)\n",
    "    \n",
    "    urltag=html.find_all('div',{\"class\":\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"})\n",
    "    for i in urltag:\n",
    "        urlhtml=i.find('span',{\"class\":\"fs-block css-ux5mu6\"})\n",
    "        for link in urlhtml.find_all('a'):\n",
    "            urlname=link.get('href')\n",
    "            urllist.append(urlname)\n",
    "        \n",
    "        \n",
    "        #for link in urlhtml.find_all('a'):\n",
    "        \n",
    "            #links=[link.get('href') for link in urlhtml]\n",
    "        #links=urlhtml.div.div.div.get('href')\n",
    "        #print(links)\n",
    "            #print(len(links))\n",
    "            #url.append(links)#url.append(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Fritto_Data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to datetime\n",
    "df['Date Posted'] = pd.to_datetime(df['Date Posted']).dt.date\n",
    "df['Month Posted'] = df['Date Posted'].apply(lambda x: x.strftime('%m'))\n",
    "df['Month_Year Posted'] = df['Date Posted'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "df['Year Posted']=df['Date Posted'].apply(lambda x: x.strftime('%Y'))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Target\"] = np.where(df[\"Ratings\"] >= 4, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Fritto Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, quote_plus\n",
    "standard_yelp_url='https://www.yelp.com'\n",
    "\n",
    "new_url=[]\n",
    "for i in df['Reviewer URL']:\n",
    "    url_final=urljoin(standard_yelp_url,i)\n",
    "    new_url.append(url_final)\n",
    "    \n",
    "df['Reviewer URL']=new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Fritto_cust_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from urllib.parse import urljoin, quote_plus\n",
    "standard_yelp_url='https://www.yelp.com'\n",
    "test=urljoin(standard_yelp_url,df['Reviewer URL'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Reviews Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Processing \n",
    "\n",
    "#Bad Reviews:\n",
    "df_bad=df[df['Target']==0]\n",
    "#Good Reviews:\n",
    "df_good=df[df['Target']==1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud - Good reviews\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text=\"\"\n",
    "for review in df_good['Reviews']:\n",
    "    text=text+review\n",
    "    \n",
    "text=text.lower()\n",
    "wordcloud=WordCloud().generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "\n",
    "wordcloud=WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud - Bad reviews\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text=\"\"\n",
    "for review in df_bad['Reviews']:\n",
    "    text=text+review\n",
    "    \n",
    "text=text.lower()\n",
    "wordcloud=WordCloud().generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "\n",
    "wordcloud=WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar Plot of Rating Breakdown\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "axes1 = plt.subplot(2,2,1)\n",
    "axes1 = sns.countplot(x='Ratings', data=df)\n",
    "axes1.set_title('Ratings')\n",
    "axes1.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Reviews per Month\n",
    "import matplotlib.pyplot as plt\n",
    "df['Date Posted'] = pd.to_datetime(df['Date Posted'])\n",
    "df = df.set_index('Date Posted')\n",
    "plt.plot(df['Reviews'].resample('M').count())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Number of reviews per month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot shows the distribution of monthly reviews - aggregate average\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df['Month Posted'], color = 'darkcyan')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "\n",
    "#Summer months seem to be popular for reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022 Most popular months for reviews\n",
    "df_2022=df[df['Year Posted']=='2022']\n",
    "plt.rcParams['figure.figsize'] = [35, 18]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df_2022['Month Posted'], color = 'blue')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Most popular months for reviews\n",
    "df_2021=df[df['Year Posted']=='2021']\n",
    "plt.rcParams['figure.figsize'] = [35, 18]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df_2021['Month Posted'], color = 'red')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020 Most popular months for reviews\n",
    "df_2020=df[df['Year Posted']=='2020']\n",
    "plt.rcParams['figure.figsize'] = [35, 18]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df_2020['Month Posted'], color = 'green')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Most popular months for reviews\n",
    "df_2019=df[df['Year Posted']=='2019']\n",
    "plt.rcParams['figure.figsize'] = [35, 18]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df_2019['Month Posted'], color = 'yellow')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot shows the distribution of monthly reviews\n",
    "plt.rcParams['figure.figsize'] = [35, 18]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_year = sns.countplot(df['Month_Year Posted'], color = 'darkcyan')\n",
    "sns_year.set(xlabel = \"Month\", ylabel = \"Count\", title = \"Distribution of the Reviews according to the Month\")\n",
    "plt.xticks(rotation = 90)\n",
    "#Reviews seem to be increasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Customer Review per Month - aggregate average\n",
    "plt.plot(df['Ratings'].resample('M').mean())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Average Monthly Customer Rating')\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022 Average Monthly Customer Rating\n",
    "plt.plot(df_2022['Ratings'].resample('M').mean())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Average Monthly Customer Rating')\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2021 Average Monthly Customer Rating\n",
    "plt.plot(df_2021['Ratings'].resample('M').mean())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Average Monthly Customer Rating')\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020 Average Monthly Customer Rating\n",
    "plt.plot(df_2020['Ratings'].resample('M').mean())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Average Monthly Customer Rating')\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019 Average Monthly Customer Rating\n",
    "plt.plot(df_2019['Ratings'].resample('M').mean())\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Average Monthly Customer Rating')\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Reviews Additional Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()\n",
    "stemmer=nltk.SnowballStemmer('english')\n",
    "stop_words=set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contractions List\n",
    "contractions_dict = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"doesn’t\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"don’t\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y’all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"ain’t\": \"am not\",\n",
    "\"aren’t\": \"are not\",\n",
    "\"can’t\": \"cannot\",\n",
    "\"can’t’ve\": \"cannot have\",\n",
    "\"’cause\": \"because\",\n",
    "\"could’ve\": \"could have\",\n",
    "\"couldn’t\": \"could not\",\n",
    "\"couldn’t’ve\": \"could not have\",\n",
    "\"didn’t\": \"did not\",\n",
    "\"doesn’t\": \"does not\",\n",
    "\"don’t\": \"do not\",\n",
    "\"don’t\": \"do not\",\n",
    "\"hadn’t\": \"had not\",\n",
    "\"hadn’t’ve\": \"had not have\",\n",
    "\"hasn’t\": \"has not\",\n",
    "\"haven’t\": \"have not\",\n",
    "\"he’d\": \"he had\",\n",
    "\"he’d’ve\": \"he would have\",\n",
    "\"he’ll\": \"he will\",\n",
    "\"he’ll’ve\": \"he will have\",\n",
    "\"he’s\": \"he is\",\n",
    "\"how’d\": \"how did\",\n",
    "\"how’d’y\": \"how do you\",\n",
    "\"how’ll\": \"how will\",\n",
    "\"how’s\": \"how is\",\n",
    "\"i’d\": \"i would\",\n",
    "\"i’d’ve\": \"i would have\",\n",
    "\"i’ll\": \"i will\",\n",
    "\"i’ll’ve\": \"i will have\",\n",
    "\"i’m\": \"i am\",\n",
    "\"i’ve\": \"i have\",\n",
    "\"isn’t\": \"is not\",\n",
    "\"it’d\": \"it would\",\n",
    "\"it’d’ve\": \"it would have\",\n",
    "\"it’ll\": \"it will\",\n",
    "\"it’ll’ve\": \"it will have\",\n",
    "\"it’s\": \"it is\",\n",
    "\"let’s\": \"let us\",\n",
    "\"ma’am\": \"madam\",\n",
    "\"mayn’t\": \"may not\",\n",
    "\"might’ve\": \"might have\",\n",
    "\"mightn’t\": \"might not\",\n",
    "\"mightn’t’ve\": \"might not have\",\n",
    "\"must’ve\": \"must have\",\n",
    "\"mustn’t\": \"must not\",\n",
    "\"mustn’t’ve\": \"must not have\",\n",
    "\"needn’t\": \"need not\",\n",
    "\"needn’t’ve\": \"need not have\",\n",
    "\"o’clock\": \"of the clock\",\n",
    "\"oughtn’t\": \"ought not\",\n",
    "\"oughtn’t’ve\": \"ought not have\",\n",
    "\"shan’t\": \"shall not\",\n",
    "\"sha’n’t\": \"shall not\",\n",
    "\"shan’t’ve\": \"shall not have\",\n",
    "\"she’d\": \"she would\",\n",
    "\"she’d’ve\": \"she would have\",\n",
    "\"she’ll\": \"she will\",\n",
    "\"she’ll’ve\": \"she will have\",\n",
    "\"she’s\": \"she is\",\n",
    "\"should’ve\": \"should have\",\n",
    "\"shouldn’t\": \"should not\",\n",
    "\"shouldn’t’ve\": \"should not have\",\n",
    "\"so’ve\": \"so have\",\n",
    "\"so’s\": \"so is\",\n",
    "\"that’d\": \"that would\",\n",
    "\"that’d’ve\": \"that would have\",\n",
    "\"that’s\": \"that is\",\n",
    "\"there’d\": \"there would\",\n",
    "\"there’d’ve\": \"there would have\",\n",
    "\"there’s\": \"there is\",\n",
    "\"they’d\": \"they would\",\n",
    "\"they’d’ve\": \"they would have\",\n",
    "\"they’ll\": \"they will\",\n",
    "\"they’ll’ve\": \"they will have\",\n",
    "\"they’re\": \"they are\",\n",
    "\"they’ve\": \"they have\",\n",
    "\"to’ve\": \"to have\",\n",
    "\"wasn’t\": \"was not\",\n",
    "\"we’d\": \"we would\",\n",
    "\"we’d’ve\": \"we would have\",\n",
    "\"we’ll\": \"we will\",\n",
    "\"we’ll’ve\": \"we will have\",\n",
    "\"we’re\": \"we are\",\n",
    "\"we’ve\": \"we have\",\n",
    "\"weren’t\": \"were not\",\n",
    "\"what’ll\": \"what will\",\n",
    "\"what’ll’ve\": \"what will have\",\n",
    "\"what’re\": \"what are\",\n",
    "\"what’s\": \"what is\",\n",
    "\"what’ve\": \"what have\",\n",
    "\"when’s\": \"when is\",\n",
    "\"when’ve\": \"when have\",\n",
    "\"where’d\": \"where did\",\n",
    "\"where’s\": \"where is\",\n",
    "\"where’ve\": \"where have\",\n",
    "\"who’ll\": \"who will\",\n",
    "\"who’ll’ve\": \"who will have\",\n",
    "\"who’s\": \"who is\",\n",
    "\"who’ve\": \"who have\",\n",
    "\"why’s\": \"why is\",\n",
    "\"why’ve\": \"why have\",\n",
    "\"will’ve\": \"will have\",\n",
    "\"won’t\": \"will not\",\n",
    "\"won’t’ve\": \"will not have\",\n",
    "\"would’ve\": \"would have\",\n",
    "\"wouldn’t\": \"would not\",\n",
    "\"wouldn’t’ve\": \"would not have\",\n",
    "\"y’all\": \"you all\",\n",
    "\"y’all\": \"you all\",\n",
    "\"y’all’d\": \"you all would\",\n",
    "\"y’all’d’ve\": \"you all would have\",\n",
    "\"y’all’re\": \"you all are\",\n",
    "\"y’all’ve\": \"you all have\",\n",
    "\"you’d\": \"you would\",\n",
    "\"you’d’ve\": \"you would have\",\n",
    "\"you’ll\": \"you will\",\n",
    "\"you’ll’ve\": \"you will have\",\n",
    "\"you’re\": \"you are\",\n",
    "\"you’re\": \"you are\",\n",
    "\"you’ve\": \"you have\",\n",
    "}\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the Tweets\n",
    "def clean_tweet(tweet):\n",
    "    tweet=str(tweet.lower())\n",
    "    tweet = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweet)\n",
    "    tweet = re.sub('\\$[a-zA-Z0-9]*', ' ', tweet)\n",
    "    tweet = re.sub('\\@[a-zA-Z0-9]*', ' ', tweet)\n",
    "    tweet = re.sub('[^a-zA-Z\\']', ' ', tweet)\n",
    "    tweet = expand_contractions(tweet)\n",
    "    tweet = tweet.replace(\"'s\", \"\")\n",
    "    tweet = tweet.replace(\"’s\", \"\")\n",
    "    tweet = tweet.replace(\"\\'s\", \"\")\n",
    "    tweet = tweet.replace(\"\\’s\", \"\")\n",
    "    tweet=re.sub(' +', ' ',tweet)\n",
    "    tweet = re.sub(' +', ' ',tweet)\n",
    "    tweet = tweet.join(word for word in tweet if word not in punctuation)\n",
    "    tweet = re.sub(' +', ' ',tweet)\n",
    "    tweet = [word for word in tweet.split(' ') if word not in stop_words]\n",
    "    tweet = \" \".join(tweet)\n",
    "    tweet = [stemmer.stem(word) for word in tweet.split(' ')]\n",
    "    tweet =\" \".join(tweet)\n",
    "    tweet = tweet[1:]\n",
    "\n",
    "    tweet=' '.join([lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) if x not in stop_words])\n",
    "    tweet=[lemma.lemmatize(x,nltk.corpus.reader.wordnet.VERB) for x in nltk.wordpunct_tokenize(tweet) if x not in stop_words]\n",
    "    return tweet\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean Review']=df['Reviews'].apply(clean_tweet)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('fritto_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('fritto_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    keywords = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    keywords = [lemmatizer.lemmatize(keyword) for keyword in keywords]\n",
    "    keyword_counts = Counter(keywords)\n",
    "    return [keyword for keyword, count in keyword_counts.most_common(5)]\n",
    "\n",
    "def analyze_sentiment(review):\n",
    "    blob = TextBlob(review)\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    if sentiment_polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif sentiment_polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    return sentiment, sentiment_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting positive and negative keeywords\n",
    "positive_keywords = []\n",
    "negative_keywords = []\n",
    "for review in df['Clean Review']:\n",
    "    sentiment, polarity = analyze_sentiment(review)\n",
    "    keywords = extract_keywords(review)\n",
    "    if sentiment == 'positive':\n",
    "        positive_keywords.extend(keywords)\n",
    "    else:\n",
    "        negative_keywords.extend(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common positive keywords:', Counter(positive_keywords).most_common(5))\n",
    "print('Most common negative keywords:', Counter(negative_keywords).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Positive Review Keywords\n",
    "positive_reviews_df = df[df[\"Target\"] == 1]\n",
    "positive_reviews = \" \".join(positive_reviews_df[\"Clean Review\"].tolist())\n",
    "positive_reviews_tokens = word_tokenize(positive_reviews)\n",
    "positive_reviews_keywords = Counter(positive_reviews_tokens).most_common(10)\n",
    "print(\"Positive reviews keywords:\")\n",
    "print(positive_reviews_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Review Keywords\n",
    "negative_reviews_df = df[df[\"Target\"] == 0]\n",
    "negative_reviews = \" \".join(negative_reviews_df[\"Clean Review\"].tolist())\n",
    "negative_reviews_tokens = word_tokenize(negative_reviews.lower())\n",
    "negative_reviews_keywords = Counter(negative_reviews_tokens).most_common(10)\n",
    "print(\"Negative review keywords:\")\n",
    "print(negative_reviews_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "for review in positive_reviews_df['Clean Review']:\n",
    "    for word in review:\n",
    "        stemmed_token_dist = FreqDist(word)\n",
    "        stemmed_dist = pd.DataFrame(stemmed_token_dist.most_common(20),columns=['Word', 'Frequency'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in negative_reviews_df['Clean Review']:\n",
    "    for word in review:\n",
    "        stemmed_token_dist = FreqDist(word)\n",
    "        stemmed_dist = pd.DataFrame(stemmed_token_dist.most_common(20),columns=['Word', 'Frequency'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "df_good=df[df['Ratings']==5]\n",
    "\n",
    "good_reviews = ' '.join(df_good['Clean Review'])\n",
    "# split the long string into sentences\n",
    "sentences_good = sent_tokenize(good_reviews)\n",
    "good_token_clean = list()\n",
    "# get tokens for each sentence\n",
    "for sentence in sentences_good:\n",
    "    eng_word = re.findall(r'[A-Za-z\\-]+', sentence)\n",
    "    good_token_clean.append([i.lower() for i in eng_word if i.lower() not in stop_words])\n",
    "    \n",
    "good_token_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model_ted = Word2Vec(sentences=good_token_clean, size=500, window=10, min_count=1, workers=4, sg=0)\n",
    "model_ted.predict_output_word(['service'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = df[\"Clean Review\"]\n",
    "y = df[\"Target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def words_and_char_bigrams(text):\n",
    "    words = re.findall(r'\\w{3,}', text)\n",
    "    for w in words:\n",
    "        yield w\n",
    "        for i in range(len(w) - 2):\n",
    "            yield w[i:i+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=words_and_char_bigrams).fit(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bow_transformer.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SMOTE the training data\n",
    "sm = SMOTE(random_state=1)\n",
    "X_bal, y_bal = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_bal, y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model on Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Review Prediction\n",
    "positive_review = df['Reviews'][59]\n",
    "positive_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_transformed = bow_transformer.transform([positive_review])\n",
    "nb.predict(positive_review_transformed)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Review Prediction\n",
    "negative_review = df['Reviews'][309]\n",
    "negative_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_review_transformed = bow_transformer.transform([negative_review])\n",
    "nb.predict(negative_review_transformed)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarization algorithm comparison\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def summarize_review(review_text, max_sentences):\n",
    "    parser = PlaintextParser.from_string(review_text, Tokenizer(\"english\"))\n",
    "    \n",
    "    # Try using different summarization algorithms\n",
    "    lsa_summarizer = LsaSummarizer()\n",
    "    luhn_summarizer = LuhnSummarizer()\n",
    "    lexrank_summarizer = LexRankSummarizer()\n",
    "\n",
    "    lsa_summary = lsa_summarizer(parser.document, sentences_count=max_sentences)\n",
    "    luhn_summary = luhn_summarizer(parser.document, sentences_count=max_sentences)\n",
    "    lexrank_summary = lexrank_summarizer(parser.document, sentences_count=max_sentences)\n",
    "\n",
    "    # Choose the best summary among different algorithms\n",
    "    summaries = [lsa_summary, luhn_summary, lexrank_summary]\n",
    "    best_summary = max(summaries, key=len)\n",
    "\n",
    "    return \" \".join([str(sentence) for sentence in best_summary])\n",
    "\n",
    "# Example Yelp restaurant review\n",
    "restaurant_review = df['Reviews'][1]\n",
    "\n",
    "# Generate summary for the review with a maximum of 3 sentences\n",
    "max_sentences = 3\n",
    "summary = summarize_review(restaurant_review, max_sentences)\n",
    "print(\"Original Review:\\n\", restaurant_review)\n",
    "print(\"\\nGenerated Summary (up to 3 sentences):\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aspect-based text summarization - can be used for next step: keyword review classification\n",
    "import spacy\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def aspect_based_summarization(review_text):\n",
    "    # Process the review text using spaCy\n",
    "    doc = nlp(review_text)\n",
    "\n",
    "    # Initialize aspect-specific summaries\n",
    "    aspect_summaries = {\n",
    "        \"food quality\": [],\n",
    "        \"service\": [],\n",
    "        \"ambiance\": [],\n",
    "        \"prices\": [],\n",
    "        \"overall\": []\n",
    "    }\n",
    "\n",
    "    # Define aspect keywords\n",
    "    aspect_keywords = {\n",
    "        \"food quality\": ['quality','lemon','runny','lime','cuisine','takeout','vegan','gnocchi','vegetarian','fresh','well-prepared','stale','well','prepared','authentic','atomic','calamari','pillows','puffs','cheese','ricotta','dish','dessert','inedible','flourless','tiramisu','satisfying','caprese','turkey','steak','chicken','sausage','lasagna','jumbo','like','pasta','salt','bread','shrimp','chocolate','cake','cream','drinks','wine', \"delicious\",'taste', \"tasty\", \"flavorful\",'runny','flavorless','flavor',\"fresh\",'disgusting','penne','fettuccine','alfredo', \"high-quality\",'prepared', \"well-prepared\",'garlic','bland','unflavorful','oily','oil','greasy','cream','heavy','sauce','fried','filling','sauces','ravioli'],\n",
    "        \"service\": ['host','listen','listening','delivery','call','wait','right away','service','slow','time','menu','hostess','table','seat','host','waiter','table','receptionist','quick','responsive','accommodating','waitress','seated','rude','took','order','ordering',\"service\",'min','minutes','server', \"excellent\",'check', \"staff\", \"attentive\",\"friendly\", \"helpful\",'courteous', \"prompt\",'mean','answer', \"efficient\", \"knowledgeable\",'phone','polite'],\n",
    "        \"ambiance\": [\"ambiance\",'clean','dirty','bathrooms','decor','atmosphere','temperature','AC','tables','toilet','kitchen','party','place','family','casual','formal','romantic','spot','festive','lively','intimate','loud','noisy','bar','seats','chairs','location', \"atmosphere\", \"decor\", \"cozy\", \"inviting\", \"warm\", \"stylish\", \"elegant\", \"relaxed\",'parking','building','seating','park','lights'],\n",
    "        \"prices\": [\"prices\",'price','total','bill was','check was','worth','value','money','pricey','pricy','portions','portion size','quantity', \"reasonable\",'over-priced','over','priced','loaded', \"affordable\",'inexpensive','wallet', \"cost\", \"expensive\", \"cheap\", \"wallet-friendly\",\"large portions\",\"large plate\"],\n",
    "        \"overall\": ['overall','fritto misto','santa monica','la','los angeles','card','somewhere different','somewhere else','course','refund','reviews','review',\"best spot\",'food','come again','avoid','meal','menu','back','return','choice','elsewhere', \"experience\", \"visit\",'would not return', \"would return\",'come back','place','italian','restaurant','website','app', \"would recommend to a friend\", \"would return\", \"would not return\",'people','crowd','italian','authentic','waste']\n",
    "    }\n",
    "\n",
    "    # Extract sentences related to each aspect\n",
    "    for sentence in doc.sents:\n",
    "        for aspect, keywords in aspect_keywords.items():\n",
    "            for word in keywords:\n",
    "                if word in sentence.text.lower():\n",
    "                    aspect_summaries[aspect].append(sentence.text)\n",
    "                    break\n",
    "\n",
    "    # Generate summaries for each aspect\n",
    "    for aspect, sentences in aspect_summaries.items():\n",
    "        if sentences:\n",
    "            aspect_summaries[aspect] = \" \".join(sentences)\n",
    "        else:\n",
    "            aspect_summaries[aspect] = \"No specific comments related to this aspect.\"\n",
    "\n",
    "    return aspect_summaries\n",
    "\n",
    "# Example Yelp restaurant review\n",
    "restaurant_review = df['Clean Review'][1]\n",
    "\n",
    "# Generate aspect-based summaries for the review\n",
    "aspect_summaries = aspect_based_summarization(restaurant_review)\n",
    "\n",
    "# Print aspect-based summaries\n",
    "for aspect, summary in aspect_summaries.items():\n",
    "    print(f\"{aspect.capitalize()} Summary:\\n{summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
